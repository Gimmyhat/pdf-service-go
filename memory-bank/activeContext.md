# Active Context - PDF Service Go

## Текущий фокус работы

**Повышение устойчивости сервиса и статистики: устранены рестарты, добавлены ретраи инициализации БД, исправлено отображение Архива/статистики; стандартизирован деплой через уникальные теги**

## Недавние изменения (Август 2025)

### ✅ Реализована система детального анализа запросов

- **Новые таблицы БД**:
  - `error_logs` - детальная информация об ошибках с stack traces
  - `request_details` - полное логирование HTTP запросов (headers, body, IP, duration)
- **Go компоненты**:
  - `RequestCaptureMiddleware` - перехват всех HTTP запросов
  - `RequestAnalysisHandler` - API для анализа запросов
  - `RequestDetail`, `RequestCapture` структуры для работы с данными
- **API endpoints для анализа**:
  - `GET /api/v1/requests/error` - список запросов с ошибками
  - `GET /api/v1/requests/analytics` - аналитика по запросам
  - `GET /api/v1/requests/:id` - детали конкретного запроса
  - `GET /api/v1/requests/:id/body` - тело запроса для анализа
- **Интегрированный веб-интерфейс**: `/dashboard` с единым просмотром ошибок и запросов

### ✅ Архив запросов и результатов

- На вкладке «Архив» отображаются только запросы конвертации JSON→PDF по путям: `/api/v1/docx` и `/generate-pdf`
- Сохраняются артефакты на диск в `/app/data/artifacts/{requests,results}` и раздаются через статический маршрут `/files`
- Добавлены индексы БД для ускорения выборки архива: `idx_request_details_path`, `idx_request_details_path_ts`
- Endpoint'ы архива зарегистрированы внутри группы `v1` (исправлен 404)
- Кнопка «Очистить, оставить N» удаляет старые записи и связанные файлы
- Включена пагинация (limit/offset), по умолчанию 25 записей, сервер отдаёт `has_more` без дорогостоящего COUNT(*)
- Автоочистка до последних 100 записей запускается асинхронно при первой загрузке, чтобы не блокировать ответ
- Реализована бесконечная прокрутка (IntersectionObserver) для плавной подгрузки страниц

### ✅ Улучшена автоматизация Makefile

- **Убраны жестко прописанные IP**: Автоматическое определение URL сервиса
- **Централизованные переменные**: `REGISTRY_PROFILE` (mirror|devops|nexus)
- **Новые/обновлённые команды**:
  - `make new-version` — генерирует уникальный тег YY.MM.DD.HHMM, пушит в RW и записывает `current_version.txt`
  - `make deploy ENV=test|prod` — деплой через Helm; версия берётся из `current_version.txt` если `VERSION` не задана
  - `make force-update ENV=...` — kubectl set image только для сервиса (без Helm), берёт версию из `current_version.txt`, если не задана
  - `make get-service-url`, `make test-error-system`, `make show-mirror-usage`, `make update-mirror NEW_MIRROR=...`

### ✅ Решены проблемы развертывания

- **Docker registry workflow**: Push в Nexus RW (`registry-irk-rw.devops.rgf.local`) → автоматическая репликация в DevOps RO (`registry.devops.rgf.local`) по префиксу `rgf.irk.*` → Pull в кластере анонимно по уникальному тегу
- **Стандартизирован профиль реестра**: `REGISTRY_PROFILE=mirror|devops|nexus` в Makefile (по умолчанию `devops`)
- **Анонимный pull в RO**: Убраны `imagePullSecrets` из Helm values (`values.yaml`, `values-test.yaml`, `values-prod.yaml`)
- **Принудительное обновление**: `make force-update` для точечного обновления только сервиса

### ✅ Таймауты и устойчивость

- Введена переменная окружения `REQUEST_TIMEOUT` (по умолчанию 180s) — используется в middleware и как `WriteTimeout` сервера
- Увеличены таймауты клиента Gotenberg и пробы k8s; добавлен `startupProbe` для `nas-pdf-service`
- Включено gzip-сжатие ответов для ускорения отдачи больших JSON
- Инициализация статистики переведена на безопасные ретраи в фоне (`InitializeOrRetry`) вместо фатального выхода
- Хендлеры статистики/ошибок/архива теперь лениво получают актуальный `PostgresDB`/`Statistics` при каждом запросе (устранён 503/500 после старта)

## Активные решения

### Архитектурные решения

- **Микросервисная архитектура** с отдельными компонентами для каждой функции
- **Event-driven error tracking** с асинхронным логированием ошибок
- **Graceful degradation** при недоступности внешних сервисов

### Технические решения

- **Structured logging** с Zap для машиночитаемых логов
- **OpenTelemetry tracing** для end-to-end видимости запросов
- **PostgreSQL для persistence** статистики и ошибок
- **Circuit breaker pattern** для устойчивости к сбоям

### Операционные решения

- **Makefile-based automation** для всех операций развертывания
- **Environment-specific configuration** через переменные и контексты
- **Health checks и probes** для Kubernetes

### ✅ Оптимизирована стабильность сервисов

- **Увеличены ресурсы**:
  - Gotenberg: CPU 750m→1000m, Memory 1Gi→1.5Gi (requests), CPU 1500m→2000m, Memory 2Gi→3Gi (limits)
  - PostgreSQL: CPU 500m→750m, Memory 1Gi→1.5Gi (requests), CPU 1000m→1500m, Memory 2Gi→3Gi (limits)
- **Оптимизированы Health Checks**: Увеличены интервалы и количество попыток для снижения ложных рестартов
- **Настройки для NFS storage**: Специальная конфигурация PostgreSQL для стабильной работы с сетевым хранилищем
- **Решена проблема request_id**: Исправлена генерация корректных ID без символов кодировки

## Следующие шаги

### ✅ Завершено: Единый интерфейс мониторинга

- ✅ Создан унифицированный `/dashboard` с вкладками Обзор/Статистика/Ошибки
- ✅ Интегрирован просмотр тела запросов прямо в веб-интерфейсе (исправлены «undefined» в модалке)
- ✅ Реализованы модальные окна с деталями запросов, копированием и форматированием JSON
- ✅ Комбинированное отображение старых и новых ошибок в едином списке
- ✅ Автоматическое создание схемы БД при развертывании для production-ready решения

### Приоритет 1: Интеграции и уведомления

- Настройка Slack/email уведомлений для критических алертов
- SLA метрики и reporting дашборды
- Интеграция с внешними системами мониторинга

### Приоритет 2: Производительность

- Оптимизация обработки больших документов
- Кэширование часто используемых шаблонов
- Асинхронная обработка для длительных операций

### Приоритет 3: Безопасность

- Аутентификация и авторизация API
- Rate limiting для предотвращения abuse
- Аудит логи для compliance

## Текущие известные проблемы

- **PostgreSQL на NFS**: остаётся риском для надёжности (I/O фризы, recovery). Рекомендация: миграция на блоковое хранилище и оператор (CloudNativePG/Zalando) или внешняя БД
- **DNS флапы**: рассмотреть включение NodeLocal DNSCache для снижения `server misbehaving`
- **Синхронизация зеркала**: Задержка 15-30 минут при обновлении образов

## Контекст для будущих изменений

- Система готова к горизонтальному масштабированию
- Архитектура поддерживает добавление новых типов документов
- Monitoring stack готов к расширению метрик
- Error tracking может быть интегрирован с внешними системами (Sentry, etc.)
