apiVersion: monitoring.coreos.com/v1
kind: PrometheusRule
metadata:
  name: nas-pdf-service-alerts
  namespace: print-serv
  labels:
    app: nas-pdf-service
    prometheus: k8s
    role: alert-rules
spec:
  groups:
  - name: circuit-breaker
    rules:
    - alert: NasPdfServiceCircuitBreakerOpen
      expr: circuit_breaker_state{name=~"gotenberg|docx_generator", job="pdf-service"} == 1
      for: 5m
      labels:
        severity: warning
      annotations:
        summary: "Circuit Breaker is Open"
        description: "Circuit Breaker for {{ $labels.name }} in pod {{ $labels.pod_name }} has been Open for 5 minutes"
        
    - alert: NasPdfServiceHighCircuitBreakerFailureRate
      expr: rate(circuit_breaker_failures_total{name=~"gotenberg|docx_generator", job="pdf-service"}[5m]) > 0.1
      for: 5m
      labels:
        severity: warning
      annotations:
        summary: "High Circuit Breaker failure rate"
        description: "Circuit Breaker for {{ $labels.name }} in pod {{ $labels.pod_name }} has high failure rate"
        
    - alert: NasPdfServiceCircuitBreakerUnhealthy
      expr: circuit_breaker_pod_health{name=~"gotenberg|docx_generator", job="pdf-service"} == 0
      for: 5m
      labels:
        severity: critical
      annotations:
        summary: "Circuit Breaker is Unhealthy"
        description: "Circuit Breaker health check failing for {{ $labels.name }} in pod {{ $labels.pod_name }}"
        
    - alert: NasPdfServiceCircuitBreakerSlowRecovery
      expr: histogram_quantile(0.95, rate(circuit_breaker_recovery_duration_seconds_bucket{name=~"gotenberg|docx_generator", job="pdf-service"}[30m])) > 300
      for: 15m
      labels:
        severity: warning
      annotations:
        summary: "Circuit Breaker slow recovery"
        description: "Circuit Breaker for {{ $labels.name }} in pod {{ $labels.pod_name }} is taking too long to recover"

  - name: pdf-service.scaling
    rules:
    - alert: HighCPUUsage
      expr: avg(rate(container_cpu_usage_seconds_total{container="nas-pdf-service"}[5m])) * 100 > 60
      for: 5m
      labels:
        severity: warning
      annotations:
        summary: High CPU usage on PDF Service
        description: PDF Service CPU usage is above 60% for 5 minutes

    - alert: HighMemoryUsage
      expr: avg(container_memory_usage_bytes{container="nas-pdf-service"}) / avg(container_memory_limit_bytes{container="nas-pdf-service"}) * 100 > 65
      for: 5m
      labels:
        severity: warning
      annotations:
        summary: High Memory usage on PDF Service
        description: PDF Service Memory usage is above 65% for 5 minutes

    - alert: HighErrorRate
      expr: sum(increase(pdf_generation_total{status="error"}[5m])) / sum(increase(pdf_generation_total[5m])) * 100 > 10
      for: 5m
      labels:
        severity: warning
      annotations:
        summary: High error rate in PDF generation
        description: PDF generation error rate is above 10% in the last 5 minutes

    - alert: ScalingLimited
      expr: kube_horizontalpodautoscaler_status_condition{condition="ScalingLimited",namespace="print-serv",status="true"} == 1
      for: 10m
      labels:
        severity: warning
      annotations:
        summary: HPA scaling is limited
        description: HorizontalPodAutoscaler is unable to scale further for 10 minutes

    - alert: SlowResponseTime
      expr: histogram_quantile(0.95, sum(rate(pdf_generation_duration_seconds_bucket[5m])) by (le)) > 3
      for: 5m
      labels:
        severity: warning
      annotations:
        summary: Slow PDF generation response time
        description: 95th percentile of PDF generation time is above 3 seconds for 5 minutes

    - alert: PDFServiceLowCacheHitRate
      expr: |
        sum(rate(template_cache_hits_total[5m])) /
        (sum(rate(template_cache_hits_total[5m])) + sum(rate(template_cache_misses_total[5m]))) < 0.7
      for: 15m
      labels:
        severity: warning
      annotations:
        summary: "Низкий процент попаданий в кэш шаблонов"
        description: "Hit rate кэша шаблонов ниже 70% в течение 15 минут"

    - alert: PDFServiceHighCacheSize
      expr: sum(template_cache_size_bytes) > 100 * 1024 * 1024
      for: 5m
      labels:
        severity: warning
      annotations:
        summary: "Большой размер кэша шаблонов"
        description: "Размер кэша шаблонов превышает 100MB"

    - alert: PDFServiceHighCacheMissRate
      expr: |
        sum(rate(template_cache_misses_total[5m])) /
        (sum(rate(template_cache_hits_total[5m])) + sum(rate(template_cache_misses_total[5m]))) > 0.4
      for: 10m
      labels:
        severity: warning
      annotations:
        summary: "Высокая частота промахов кэша"
        description: "Более 40% запросов не находят шаблон в кэше"

    - alert: PDFServiceUnstableCacheSize
      expr: |
        abs(
          (sum(template_cache_size_bytes) - 
          sum(template_cache_size_bytes offset 5m)) /
          sum(template_cache_size_bytes offset 5m)
        ) > 0.3
      for: 15m
      labels:
        severity: warning
      annotations:
        summary: "Нестабильный размер кэша"
        description: "Размер кэша изменяется более чем на 30% за 5 минут"

    - alert: PDFServiceCacheMemoryPressure
      expr: |
        sum(template_cache_size_bytes) / 
        sum(container_memory_working_set_bytes{container="pdf-service"}) * 100 > 40
      for: 5m
      labels:
        severity: warning
      annotations:
        summary: "Высокое потребление памяти кэшем"
        description: "Кэш использует более 40% рабочей памяти сервиса"

    - alert: PDFServiceCacheEfficiencyDrop
      expr: |
        (
          sum(rate(template_cache_hits_total[30m])) /
          (sum(rate(template_cache_hits_total[30m])) + sum(rate(template_cache_misses_total[30m])))
        ) < 0.5
      for: 30m
      labels:
        severity: warning
      annotations:
        summary: "Низкая эффективность кэша"
        description: "Эффективность кэша ниже 50% в течение 30 минут"

  - name: tracing
    rules:
    - alert: TracingExportLatencyHigh
      expr: histogram_quantile(0.95, sum(rate(otel_trace_export_duration_seconds_bucket[5m])) by (le)) > 1
      for: 5m
      labels:
        severity: warning
      annotations:
        summary: "Высокая латентность экспорта трейсов"
        description: "95-й перцентиль времени экспорта трейсов превышает 1 секунду"

    - alert: TracingQueueNearlyFull
      expr: otel_trace_span_queue_size > 1800 # 90% от максимума в 2048
      for: 5m
      labels:
        severity: warning
      annotations:
        summary: "Очередь трейсов почти заполнена"
        description: "Размер очереди трейсов превышает 90% от максимального значения"

    - alert: TracingDroppedSpans
      expr: rate(otel_trace_span_drops_total[5m]) > 0
      for: 5m
      labels:
        severity: warning
      annotations:
        summary: "Обнаружены отброшенные спаны"
        description: "Сервис отбрасывает спаны, возможно, из-за переполнения очереди"

    - alert: HighErrorRate
      expr: sum(rate(http_requests_total{status=~"5.."}[5m])) / sum(rate(http_requests_total[5m])) > 0.05
      for: 5m
      labels:
        severity: critical
      annotations:
        summary: "Высокий процент ошибок"
        description: "Более 5% запросов завершаются с ошибкой 5xx"

    - alert: HighLatency
      expr: histogram_quantile(0.95, sum(rate(http_request_duration_seconds_bucket[5m])) by (le, handler)) > 2
      for: 5m
      labels:
        severity: warning
      annotations:
        summary: "Высокая латентность запросов"
        description: "95-й перцентиль времени ответа превышает 2 секунды для {{ $labels.handler }}"

    - record: tracing:span_duration:p95
      expr: histogram_quantile(0.95, sum(rate(otel_trace_span_duration_milliseconds_bucket[5m])) by (le, name))

    - record: tracing:span_duration:p99
      expr: histogram_quantile(0.99, sum(rate(otel_trace_span_duration_milliseconds_bucket[5m])) by (le, name))

    - record: tracing:error_spans:ratio
      expr: sum(rate(otel_trace_span_events_total{status="error"}[5m])) / sum(rate(otel_trace_span_events_total[5m]))

    - record: tracing:spans:rate
      expr: sum(rate(otel_trace_span_events_total[5m])) by (name)

  - name: pdf-service-retry-alerts
    rules:
      - alert: HighRetryRate
        expr: |
          sum(rate(retry_attempts_total{status="failed"}[5m])) by (operation)
          / 
          sum(rate(retry_attempts_total{status="started"}[5m])) by (operation)
          > 0.1
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: High retry rate for {{ $labels.operation }}
          description: The retry rate for {{ $labels.operation }} is above 10% for the last 5 minutes

      - alert: LowRetrySuccessRate
        expr: |
          retry_success_rate < 0.95
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: Low retry success rate for {{ $labels.operation }}
          description: The retry success rate for {{ $labels.operation }} is below 95% for the last 5 minutes

      - alert: HighRetryLatency
        expr: |
          histogram_quantile(0.95, sum(rate(retry_operation_duration_seconds_bucket[5m])) by (le, operation))
          > 1
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: High retry latency for {{ $labels.operation }}
          description: The 95th percentile retry latency for {{ $labels.operation }} is above 1 second

      - alert: RetryExhaustion
        expr: |
          sum(rate(retry_attempts_total{status="max_attempts"}[5m])) by (operation) > 0
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: Retry exhaustion for {{ $labels.operation }}
          description: Operations are hitting maximum retry attempts for {{ $labels.operation }}

      - alert: HighErrorRate
        expr: |
          sum(rate(retry_errors_total[5m])) by (operation, error_type)
          / 
          sum(rate(retry_attempts_total{status="started"}[5m])) by (operation)
          > 0.05
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: High error rate for {{ $labels.operation }} ({{ $labels.error_type }})
          description: The error rate for {{ $labels.operation }} of type {{ $labels.error_type }} is above 5%

      - alert: RetryBackoffDelay
        expr: |
          histogram_quantile(0.95, sum(rate(retry_backoff_duration_seconds_bucket[5m])) by (le, operation))
          > 2
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: High retry backoff delay for {{ $labels.operation }}
          description: The 95th percentile retry backoff delay for {{ $labels.operation }} is above 2 seconds

      - alert: ConcurrentRetries
        expr: |
          retry_current_attempts > 10
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: High number of concurrent retries for {{ $labels.operation }}
          description: The number of concurrent retry operations for {{ $labels.operation }} is above 10

  - name: pdf-service.recording
    rules:
    - record: pdf_service:request_rate:5m
      expr: sum(rate(pdf_requests_total[5m])) by (status)
    
    - record: pdf_service:error_rate:5m
      expr: sum(rate(pdf_requests_total{status="error"}[5m])) / sum(rate(pdf_requests_total[5m])) * 100

    - record: pdf_service:generation_duration:p95
      expr: histogram_quantile(0.95, sum(rate(pdf_generation_duration_seconds_bucket[5m])) by (le))

    - record: pdf_service:generation_duration:p50
      expr: histogram_quantile(0.50, sum(rate(pdf_generation_duration_seconds_bucket[5m])) by (le))

    - record: pdf_service:retry_success_rate
      expr: sum(rate(retry_attempts_total{status="success"}[5m])) by (operation) / sum(rate(retry_attempts_total[5m])) by (operation) * 100

    - record: pdf_service:circuit_breaker_failure_rate:5m
      expr: sum(rate(circuit_breaker_failures_total[5m])) by (name)

  - name: error-tracking-alerts
    rules:
      - alert: CriticalErrorsDetected
        expr: |
          sum(
            increase(error_logs_total{severity="critical"}[5m])
          ) > 3
        for: 1m
        labels:
          severity: critical
          component: "error-tracking"
        annotations:
          summary: "Критические ошибки обнаружены в системе"
          description: "Зафиксировано {{ $value }} критических ошибок за последние 5 минут"
          runbook_url: "https://wiki.example.com/runbooks/critical-errors"

      - alert: HighErrorRate
        expr: |
          sum(rate(error_logs_total[5m])) > 0.5
        for: 5m
        labels:
          severity: warning
          component: "error-tracking"
        annotations:
          summary: "Высокий уровень ошибок в системе"
          description: "Частота ошибок составляет {{ $value }} ошибок в секунду за последние 5 минут"

      - alert: ComponentErrorSpike
        expr: |
          sum by (component) (
            rate(error_logs_total[5m])
          ) > 0.2
        for: 3m
        labels:
          severity: warning
          component: "error-tracking"
        annotations:
          summary: "Всплеск ошибок в компоненте {{ $labels.component }}"
          description: "Компонент {{ $labels.component }} генерирует {{ $value }} ошибок в секунду"

      - alert: PanicErrorsDetected
        expr: |
          sum(
            increase(error_logs_total{error_type="panic"}[10m])
          ) > 0
        for: 0m
        labels:
          severity: critical
          component: "error-tracking"
        annotations:
          summary: "Обнаружены panic ошибки"
          description: "Зафиксировано {{ $value }} panic ошибок за последние 10 минут - требуется немедленное вмешательство"
          
      - alert: DatabaseErrorsIncreasing
        expr: |
          sum(
            increase(error_logs_total{component="database"}[10m])
          ) > 5
        for: 5m
        labels:
          severity: warning
          component: "database"
        annotations:
          summary: "Увеличение ошибок базы данных"
          description: "Зафиксировано {{ $value }} ошибок базы данных за последние 10 минут"

      - alert: GotenbergTimeouts
        expr: |
          sum(
            increase(error_logs_total{component="gotenberg",error_type="timeout"}[15m])
          ) > 3
        for: 5m
        labels:
          severity: warning
          component: "gotenberg"
        annotations:
          summary: "Множественные таймауты Gotenberg"
          description: "Зафиксировано {{ $value }} таймаутов Gotenberg за последние 15 минут"

      - alert: SystemHealthDegraded
        expr: |
          (
            sum(increase(error_logs_total{severity=~"critical|high"}[1h])) > 10
          ) or (
            sum(increase(error_logs_total[1h])) > 100
          )
        for: 5m
        labels:
          severity: warning
          component: "system"
        annotations:
          summary: "Деградация состояния системы"
          description: "Система показывает признаки деградации: {{ $value }} серьезных ошибок за последний час"

      - alert: NoErrorLogsReceived
        expr: |
          (
            sum(increase(error_logs_total[1h])) == 0
          ) and (
            sum(increase(http_requests_total[1h])) > 100
          )
        for: 30m
        labels:
          severity: warning
          component: "error-tracking"
        annotations:
          summary: "Система отслеживания ошибок не получает данные"
          description: "За последний час не получено ни одной записи об ошибках при активности системы - возможна проблема с логированием" 